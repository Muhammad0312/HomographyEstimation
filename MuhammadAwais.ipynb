{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution\n",
    "Here is the solution I computed. The image below shows the drone image stitched with the base map.  \n",
    "\n",
    "![WarpedImage](assets/warped.png)  \n",
    "\n",
    "The position of drone relative to the base map is X: [175.46782683] Y: [-51.13010725] Z: [0.85639727] pixels.  \n",
    "This translation is upto a scale factor.  \n",
    "The rotation with respect to the base map is [261.62]  [-0.0208]  [0.0151] following the ZYX euler angle convention.\n",
    "The positive anle being the counter clockwise direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation\n",
    "In this section I discuss how I approached this problem.  \n",
    "## Initial thoughts  \n",
    "These were my initial thoughts as saw the problem.  \n",
    "Motion between two monocular images can be described by either a fundamental matrix or a homography.  \n",
    "Fundamental matrix is better when the scene is non planar.  \n",
    "Homography is better when the scene is planar.  \n",
    "Since the drone is at an altitude of 119m, the scene can reasonably be assumed to be planar.  \n",
    "Also satellite imagery is taken at a very high altitude, it can also be assumed to be planar.  \n",
    "I would have to compute keypoints between images, match them, compute the homography and decompose  \n",
    "it to get the motion between two scenes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First approach\n",
    "Generally, I approach a problem starting forom ground up  \n",
    "So I collected some data from google earth which was simpler as compared to the given task  \n",
    "I then setup the pipeline using SIFT  \n",
    "It worked and i was able to get good stiching between images.  \n",
    "But it didn't work for the given frames which i already suspected.  \n",
    "SIFT while being invariant to scale and rotations, is not very robust against illumination changes.  \n",
    "Both images had keypoints, but there were not enough good matches to compute a good homography.  \n",
    "This resulted in a poor stitching if not a stitching at all.  \n",
    "I tried the same pipeline with ORB and AKAZE as well.  \n",
    "I have included this pipeline at the end of this notebook.  \n",
    "# Second approach\n",
    "The next approach was to look for a feature descriptor which is robust against illumination changes  \n",
    "and provides descriptors which are kind of time invariant.  \n",
    "Learning based feature detectors are known to outperform hand crafted feature detectors  \n",
    "After reviewing some literature, I chose SuperPoint as the keypoint detector and descriptor and SuperGlue as the matching algorithm.  \n",
    "I am using the model weights provided by the authors.  \n",
    "Below is my implementation of a basic visual navigation system. Comments are added where deemed necessary.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f1dc428a750>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Necessary imports\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "import torch\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from models.matching import Matching\n",
    "from models.utils import (make_matching_plot,read_image)\n",
    "\n",
    "# Turning off gradient calculation since we are in inference mode\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference on device \"cuda\"\n",
      "Loaded SuperPoint model\n",
      "Loaded SuperGlue model (\"outdoor\" weights)\n"
     ]
    }
   ],
   "source": [
    "# Configurations for SuperPoint and SuperGlue models.\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Running inference on device \\\"{}\\\"'.format(device))\n",
    "config = {\n",
    "    'superpoint': {\n",
    "        'nms_radius': 4,\n",
    "        'keypoint_threshold': 0.005,\n",
    "        'max_keypoints': 1024\n",
    "    },\n",
    "    'superglue': {\n",
    "        'weights': 'outdoor',\n",
    "        'sinkhorn_iterations': 30,\n",
    "        'match_threshold': 0.2,\n",
    "    }\n",
    "}\n",
    "\n",
    "matching = Matching(config).eval().to(device)\n",
    "# Image paths\n",
    "baseMap = 'assets/lendurai/base_map.jpg'\n",
    "dronePhoto = 'assets/lendurai/drone_photo.jpg'\n",
    "\n",
    "# Lists to store required data\n",
    "heuristic = []\n",
    "goodKp1 = []\n",
    "goodKp2 = []\n",
    "images = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heuristic is chosen based on the confidence of the number of keypoints  \n",
    "This confidence value is provided by the matching algorithm  \n",
    "Those keypoints are chosen which have the highest confidence sum  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While SuperGlue provided good matches, it didn't perform well under higher rotations  \n",
    "My approach was to rotate the drone image by a known angle, compute the matches and then account for this rotation  \n",
    "while decomposing the homography  \n",
    "I rotate the image by known rotations and select the image and keypoints with the highest value of the heuristic  \n",
    "These keypoints are then used to compute the homography  \n",
    "For homography computation, points should be spread over the whole image  \n",
    "otherwise the area having most of the points will have a good warp as compared to other region   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the base image.\n",
    "image0, inp0, scales0 = read_image(\n",
    "            baseMap, device, [-1], 0, False)\n",
    "\n",
    "for i in range(0, 4, 1):\n",
    "    image1, inp1, scales1 = read_image(\n",
    "            dronePhoto, device, [-1], i, False)\n",
    "    # Perform the matching.\n",
    "    pred = matching({'image0': inp0, 'image1': inp1})\n",
    "    pred = {k: v[0].cpu().numpy() for k, v in pred.items()}\n",
    "    # Get the matches\n",
    "    kpts0, kpts1 = pred['keypoints0'], pred['keypoints1']\n",
    "    matches, conf = pred['matches0'], pred['matching_scores0']\n",
    "\n",
    "    # Select the valid matches, keypoints which donot have a match have -1\n",
    "    valid = matches > -1\n",
    "    mkpts0 = kpts0[valid]\n",
    "    mkpts1 = kpts1[matches[valid]]\n",
    "    mconf = conf[valid]\n",
    "\n",
    "    # Save visualization\n",
    "    color = cm.jet(mconf)\n",
    "    viz_path = 'assets/viz/angle_{}.png'.format(i*90)\n",
    "    text = [\n",
    "        'SuperGlue',\n",
    "        'Keypoints: {}:{}'.format(len(kpts0), len(kpts1)),\n",
    "        'Matches: {}'.format(len(mkpts0)),\n",
    "        'Rotation: {}'.format(i*90)\n",
    "    ]\n",
    "\n",
    "    small_text = [\n",
    "                    'Matching using Superglue'\n",
    "                ]\n",
    "    \n",
    "    make_matching_plot(\n",
    "                    image0, image1, kpts0, kpts1, mkpts0, mkpts1, color,\n",
    "                    text, viz_path, False,\n",
    "                    False, False, 'Matches', small_text)\n",
    "    \n",
    "    # Choosing the matches with confidence above 0.72\n",
    "    goodMatches = mconf > 0.72\n",
    "    # Store all the good matches, heuristic and images\n",
    "    goodKp1.append(mkpts0[goodMatches])\n",
    "    goodKp2.append(mkpts1[goodMatches])\n",
    "    heuristic.append(sum(mconf[goodMatches]))\n",
    "    images.append(image1)\n",
    "\n",
    "# Select the matches with the highest heuristic and calculate the homography\n",
    "bestmatchIndex = heuristic.index(max(heuristic))\n",
    "H, _ = cv2.findHomography(np.array(goodKp2[bestmatchIndex]), np.array(goodKp1[bestmatchIndex]), cv2.RANSAC, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homography decomposition gives 4 solutions, 2 can be discarded because they have negative value of z translation  \n",
    "which means our scene is behind the camera. The third solution is discarded because rotation about the x axis is greater than  \n",
    "100 degrees and since we know that our camera is looking downwards, that solution is also not possible\n",
    "So we are left with one solution.\n",
    "\n",
    "I warp and stitch the image for better visualization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Angles:  [-8.3845048  -0.02081877  0.01513642]\n",
      "Translation:  [175.46782683] [-51.13010725] [0.85639727]\n"
     ]
    }
   ],
   "source": [
    "# Decompose the homography matrix to get the rotation and translation\n",
    "num, Rs, Ts, Ns = cv2.decomposeHomographyMat(H, np.eye(3))\n",
    "\n",
    "# Convert rotation matrices to euler angles using skikit\n",
    "r = R.from_matrix(Rs)\n",
    "angles = r.as_euler('zyx', degrees=True)\n",
    "for angles, translation in zip(angles, Ts):\n",
    "    if translation[2] > 0 and angles[2] < 135:\n",
    "        print(\"Angles: \", angles)\n",
    "        print(\"Translation: \", translation[0], translation[1], translation[2])\n",
    "\n",
    "# Warp the drone photo\n",
    "# These images are loaded independent of the implemented function\n",
    "baseMapOrig = cv2.imread(baseMap, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "h,w = images[bestmatchIndex].shape\n",
    "pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    "dst = cv2.perspectiveTransform(pts,H)\n",
    "\n",
    "height, width = baseMapOrig.shape\n",
    "dronePhotoWarped = cv2.warpPerspective(images[bestmatchIndex], H, (width, height))\n",
    "\n",
    "# Replcae the base map with the warped drone photo\n",
    "baseMapStiched = baseMapOrig.copy()\n",
    "baseMapStiched[dronePhotoWarped > 0] = dronePhotoWarped[dronePhotoWarped > 0]\n",
    "\n",
    "baseMapStiched = cv2.polylines(baseMapStiched,[np.int32(dst)],True,0,2, cv2.LINE_AA)\n",
    "\n",
    "# Display the warped drone photo\n",
    "plt.imshow(baseMapStiched, cmap='gray')\n",
    "plt.savefig('assets/warped1.png')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical applications\n",
    "I can image a complete system for drone localization, where drone position is estimated using visual odometry.  \n",
    "This approach could be used to get the global position of the drone if we have gps aligned map images  \n",
    "This could also be used in an EKF for better position estimates  \n",
    "To have a robust solution, drone images could be north aligned using the onboard IMU, if it provides compass headings.  \n",
    "This is done because the map data will be north aligned and we won't have to rotate the images from the drone and follow a heuristic based approach  \n",
    "as demonstrated above.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand crafted features pipeline\n",
    "The section below contains the pipeline developed using hand crafted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.transform import Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load Images\n",
    "baseMap  = cv2.imread('assets/lendurai/fullMapold.png', cv2.IMREAD_GRAYSCALE)\n",
    "droneImage = cv2.imread('assets/lendurai/droneRotated.png', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Resize Map\n",
    "baseMap = cv2.resize(baseMap, (300, 200))\n",
    "\n",
    "# Display Images\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].imshow(baseMap, cmap='gray')\n",
    "ax[0].set_title('Base Map')\n",
    "ax[1].imshow(droneImage, cmap='gray')\n",
    "ax[1].set_title('Drone Image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the detector, SIFT performs the best but others can also be fine tuned\n",
    "detector_name = 'SIFT'\n",
    "detector = cv2.SIFT_create()\n",
    "if detector_name == 'ORB':\n",
    "    detector = cv2.ORB_create()\n",
    "elif detector_name == 'SIFT':\n",
    "    detector = cv2.SIFT_create()\n",
    "elif detector_name == 'AKAZE':\n",
    "    detector = cv2.AKAZE_create()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the keypoints and descriptors with SIFT\n",
    "mapKeypoints, mapDescriptors = detector.detectAndCompute(baseMap, None)\n",
    "droneKeypoints, droneDescriptors = detector.detectAndCompute(droneImage, None)\n",
    "\n",
    "# Draw Keypoints\n",
    "mapKeypointsImage = cv2.drawKeypoints(baseMap, mapKeypoints, None)\n",
    "droneKeypointsImage = cv2.drawKeypoints(droneImage, droneKeypoints, None)\n",
    "\n",
    "# Display Images\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].imshow(mapKeypointsImage)\n",
    "ax[0].set_title('Map Keypoints')\n",
    "ax[1].imshow(droneKeypointsImage)\n",
    "ax[1].set_title('Drone Keypoints')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different metrics are used for different detectors\n",
    "# For sift euclidean distance is used\n",
    "# For ORB and AKAZE hamming distance is used\n",
    "if detector_name == 'ORB' or detector_name == 'AKAZE':\n",
    "    matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    matches = matcher.match(droneDescriptors, mapDescriptors)\n",
    "    matches = sorted(matches, key=lambda x: x.distance)\n",
    "    goodMatches = matches[:10]\n",
    "elif detector_name == 'SIFT':\n",
    "    # Parameters for Flann\n",
    "    FLANN_INDEX_KDTREE = 1\n",
    "    index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "    search_params = dict(checks = 50)\n",
    "\n",
    "    # Initialize Flann\n",
    "    matcher = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "    # Find the matches\n",
    "    matches = matcher.knnMatch(droneDescriptors, mapDescriptors, k=2)\n",
    "    # Store all the good matches as per Lowe's ratio test.\n",
    "    goodMatches = []\n",
    "    for m,n in matches:\n",
    "        if m.distance < 0.7*n.distance:\n",
    "            goodMatches.append(m)\n",
    "else:\n",
    "    matcher = cv2.BFMatcher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw Matches\n",
    "matchesImage = cv2.drawMatches(droneImage, droneKeypoints, baseMap, mapKeypoints, goodMatches, None)\n",
    "\n",
    "# Display Image\n",
    "plt.imshow(matchesImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the points from the good matches\n",
    "src_pts = np.float32([ droneKeypoints[m.queryIdx].pt for m in goodMatches ]).reshape(-1,1,2)\n",
    "dst_pts = np.float32([ mapKeypoints[m.trainIdx].pt for m in goodMatches ]).reshape(-1,1,2)\n",
    "\n",
    "# Find the homography matrix \n",
    "M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,2.0)\n",
    "matchesMask = mask.ravel().tolist()\n",
    "\n",
    "# Add lines for visualization \n",
    "h,w = droneImage.shape\n",
    "pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    "dst = cv2.perspectiveTransform(pts,M)\n",
    "\n",
    "# Warp the drone photo\n",
    "height, width = baseMap.shape\n",
    "dronePhotoWarped = cv2.warpPerspective(droneImage, M, (width, height))\n",
    "\n",
    "# Replace the pixels in the base map with the warped drone photo\n",
    "baseMapStiched = baseMap.copy()\n",
    "baseMapStiched[dronePhotoWarped > 0] = dronePhotoWarped[dronePhotoWarped > 0]\n",
    "\n",
    "# Display the warped drone photo\n",
    "plt.imshow(baseMapStiched, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseMap = cv2.polylines(baseMap,[np.int32(dst)],True,0,2, cv2.LINE_AA)\n",
    "draw_params = dict(matchColor = (0, 255 ,0), # draw matches in green color\n",
    " singlePointColor = None,\n",
    " matchesMask = matchesMask, # draw only inliers\n",
    " flags = 2)\n",
    " \n",
    "img3 = cv2.drawMatches(droneImage, droneKeypoints, baseMap, mapKeypoints, goodMatches,None,**draw_params)\n",
    " \n",
    "plt.imshow(img3, 'gray'),plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompose Homography to get rotation and translation\n",
    "\n",
    "retval, rvec, tvec, n = cv2.decomposeHomographyMat(M, np.eye(3))\n",
    "\n",
    "print(\"Decomposition: \", retval)\n",
    "print(\"Rotation Vector: \", rvec[0].shape)\n",
    "print(\"Translation Vector: \", tvec[0].shape)\n",
    "print(\"Normal Vector: \", n[0].shape)\n",
    "\n",
    "# Iterate over each rotation matrix and extract Euler angles\n",
    "for i, R in enumerate(rvec):\n",
    "    # Create a Rotation object from the rotation matrix\n",
    "    rotation = Rotation.from_matrix(R)\n",
    "    \n",
    "    # Convert rotation to Euler angles\n",
    "    euler_angles = rotation.as_euler('xyz', degrees=True)  # Assuming ZYX intrinsic rotations (roll, pitch, yaw)\n",
    "    \n",
    "    print(f\"Rotation Matrix {i + 1}:\")\n",
    "    print(\"Roll (X-axis rotation):\", euler_angles[0])\n",
    "    print(\"Pitch (Y-axis rotation):\", euler_angles[1])\n",
    "    print(\"Yaw (Z-axis rotation):\", euler_angles[2])\n",
    "    print(\"Translation Vector:\" , tvec[i][0], tvec[i][1], tvec[i][2])\n",
    "    print()\n",
    "\n",
    "# Displaying all decompositions, but these can be used to choose the correct one\n",
    "# as explained above, SIFT performs the best but others can be fined tuned as well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MonoGS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
